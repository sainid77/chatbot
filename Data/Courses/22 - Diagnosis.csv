 Today we will talk about diagnosis. Diagnosis is the identification of the fault or faults responsible for a malfunctioning system. The system it could be a car, a computer program, an organism, or the economy. Diagnosis builds on our discussion of classification and configuration. They start by defining diagnosis. They really setup two spaces. A data spaces and a hypothesis space. Data about the malfunctioning system. Hypothesis about the fault that can explain that malfunctioning system. Then, we'll constructing mappings through data space to hypothesis space which amount to diagnosis. We'll discuss two views of diagnosis, diagnosis as classification and diagnosis as abduction. Abduction in this context is a new term to you. We'll discuss it in more detail today.
 To illustrate the task of diagnosis, let us begin with an exercise. When we think of diagnosis, most of us think in terms of medical diagnosis. The kind of diagnosis a doctor does. So this particular exercise, is coming from medical diagnosis, actually it's a made-up exercise from medical diagnosis. And here's a set of diseases, fictional diseases, that the doctor knows about, along with the symptoms that each diseases causes. So, Alphaitis, for example, causes elevated A, reduced C and elevated F and so on. Given this set of data, and this set of diseases, what disease or set of diseases do you think the patient suffers from?
 >> That's a good answer, David. Note that David did several things in coming up with his answer. First, he made sure that his answer covers all these signs and symptoms. This is the principle of coverage. We want to make sure that the diagnostic conclusion actually accounts for all the input data. Second, we chose a single hypothesis over a combination of hypothesis, although the combination could have explain this data as well. This is the principle of parsimony. In general, we want a simple hypothesis for explaining the entire data. Third, these hypotheses can have greatest interactions between them, and these interactions can make your diagnostic task quite complicated. Fourth, they would use the term explanation. This is an important aspect for diagnosis. We want a set of hypotheses that could explain the input data. Now this [UNKNOWN] we did it in this simple exercise, because there is one single disease that can, in fact, explain all the input data. What would happen if there was no single hypothesis that could cover the entire input data? Or what would happen if there were multiple hypotheses that could equally well explain the input data? [UNKNOWN] diagnostic task can be quite complicated.
 We may define the task of diagnosis as determining what is wrong with a malfunctioning device. Or more generally, what is the fault that is responsible for a malfunctioning system. Given a system, we expect some behavior from it. We expect it to do something. However, we may observe that the system is doing something different. So there is the expected behavior and there is the observed behavior, and there is a discrepancy between them. When there is a discrepancy, we know that the system is malfunctioning. The question then becomes what is the fault or the set of faults responsible for the malfunctioning system. When we think of diagnosis we typically think of medical diagnosis. But, of course diagnosis can occur in a very large number of domains. Here are three diagnostic domains with which all of us are familiar. The first figure shows the engine of a car. When I insert the key into the ignition system, I expect the engine to turn on. That is the expected behavior. But suppose that I insert the key, and the engine doesn't turn on. That's the observed behavior. There is a discrepancy between the expected behavior and the observed behavior. When I insert the key into the ignition, I expect the engine to turn on. That's the expected behavior. But then suppose that I put the key in, and the engine doesn't turn on. That's the observed behavior. The discrepancy between the expected behavior, turn the engine on, and the observed behavior, the engine doesn't turn on, so I know there is a malfunction. Given this malfunction, the question becomes what is at fault, the force responsible for it and that's the diagnostic task. To address this diagnostic task, I may use a rule which says that if the engine doesn't turn on when the key is inserted, check the carburetor. Suppose I go and check the carburetor and everything is okay with the carburetor. Then I [INAUDIBLE] get activated. And let me say, if the engine doesn't turn on and everything is okay with the carburetor, go check the spark plugs. And this way, I am going to use a production system to isolate the fault of faults responsible for the malfunction. >> Something similar happens with computer hardware repair. When we turn on a computer, there are few behaviors that we expect of it. We expect it to boot up quickly, we expect it to run fast, and we expect it to stay cool. Now, imagine we turn a computer and notice it started to run at a much higher temperature than we're accustomed to. We might remember that the last time we encountered this problem, there were problems with the fan. So we might use that to then diagnose this a problem with the fan and replace the fan. This happens at the software level too. If I'm writing a program and the output differs from what I was expecting, I set about debugging the program and finding the fault. One way of doing that is called a rubber duck debugging, which involves explaining my model of how my program works to the rubber duck so that I might uncover the error by forcing myself through an explanation process. >> Note that we discussed the same diagnostic task in three different domains. In each domain there was a discrepancy between the expected and the observed behaviors and we tried to identify the fault, or faults responsible for it. Note also that we alluded to three different methods for doing diagnosis. The matter of rule based reasoning, the matter of case based reasoning, and the matter of model based reasoning. We haven't talked a lot about the matter of model based reasoning so far. We will do so when we come to systems thinking later in the class. Of course we can use the matter of rule based reasoning, not only for diagnosing car engines but also for repairing computer hardware or for diagnosing computer software. In this particular lesson our focus will be on the diagnostic task. By now all of us are already familiar with many reasoning methods that are potentially applicable to class.
 We can think of diagnosis as a mapping from a data space, to a hypothesis space, In case of a medical diagnosis, the data may be the greatest kind of signs and symptoms that I may go to a doctor with. Some of the data may be very specific, some of it may be very abstract, an example of a very specific data is that a [UNKNOWN] temperature is 104 degrees fahrenheit. An example of the extraction of the data is that [INAUDIBLE] is running a fever. The hypothesis space consists of all hypothesis that can explain parts of the observed data. A hypothesis in the hypothesis space can explain some part of the data, In case of medicine, this hypothesis may reference to diseases. A doctor may say that my hypothesis is that a shook is suffering from flu, and that explains his high fever. In the domain of car repairs, this hypothesis may refer to specific faults with the car, for example, the carburetor is not working properly. In the domain of computer software, this hypothesis may refer to specific methods not working properly. And this mapping from data space to the hypothesis space can be very complex. The complexity arises partly because of the size of data space, partly because of the size of hypothesis space, partly because the mapping can be M to N. And also, because this hypothesis can interact with each other, If H3 is present, H4 may be excluded, If H5 is present, H6 is sure to be present and so on. It helps then not to deal with all the raw data, but to deal with abstractions of the data, so the initial data that a patient may go to a doctor with may be very, very specific. The signs and symptoms of their particular specific patient, but the diagnostic process might abstract them from Asok has a fever of 104 degrees farenheit to Asok has a high fever. This abstract data that can be mapped into an abstract hypothesis, Asok has high fever can get mapped into Asok has a bladder infection for example. The abstract hypothesis can now be refined into a suffering from flu or a flu for a particular screen. At the end, we want a hypothesis that is as refined as possible, and that explains all the available data. When we were talking about classification, we talked about two processes of classification, bottom-up process and our top down process. The bottom up process of classification, we started with raw data and then grouped and abstracted, it in case of top down classification we started with some high level class and then established it and refined it. You can see that in diagnosis both the bottom up process of classification, and the tope down process of classification are co-occurring. This method of bottom up classification and data space, mapping and hypothesis space, and then top down classification of hypothesis space is called heuristic classification. This is yet another method like rule-based reasoning, case-based reasoning, and model-based reasoning with a diagnostic task.
 Several factors conspire to make this process of classification much more complicated. This is as you would expect in AI. If it was an easy problem it would not be part of AI. The first factor that makes this problem complicated is that one data point might be explained by multiple hypotheses. So I go to the doctor with high fever, D5 here, and several hypotheses about different diseases might explain my high fever. Which of this hypotheses, then, is true? A second factor that complicates things is that one hypotheses may explain multiple sets of data. So the hypotheses that Ashok has influenza might explain not only that he has fever, but also that he feels tired, and also that he is shivering, and also that he can't sleep at night. Go to your doctor with two data items, one that have high fever and the other that I am tired. Now the doctor may come up with a hypothesis, H3, that Ashok suffers from flu. However, when H3 is present, then one can expect other symptoms to be observed as well. However, the hypothesis of H3 may generate expectations but additional data items. How, then, may a doctor decide if H3 is true? Well, one possibility is that a doctor may ask Ashok additional questions to collect additional data. Do you shiver at night, the doctor may ask, if that is one of the expectations generated by the hypothesis of having flu. because the mapping is not only from the data space to the hypothesis space, the mapping is also from the hypothesis space to the data space. Diagnosis entails not only mapping data to hypothesis, but also to know the expectations of additional data and, collecting that additional data. Of course, both of the first two factors may be present at the same time. That is, one hypothesis may explain multiple data items. And multiple hypothesis may explain the same data item. So, in general, this is a M to M mapping, multiple hypothesis, multiple sets of data. And, of course, this immediately makes the diagnostic task harder. The fourth factor that makes the diagnostic task hard is that these hypothesis could interact with each other. One of the common interactions between hypotheses is called mutual exclusion. Mutual exclusion occurs if one hypothesis present, another hypothesis cannot be true. In this case, H3 explains D2, D3, D4. And H6 expands D of 6, D of 7, D of 8. But if H3 is present, H6 cannot be true. And if H6 is present, H3 cannot be true. This makes the diagnostic task hard because if a patient goes to a doctor with symptoms D3, D4 and D6, D7, then the question becomes whether to include H3 or to include H6 to define our conclusion. A fifth factor that makes the diagnostic test hard is called cancellation. Cancellation occurs when two hypotheses interact relative to a particular data item. As an example, I may have flu, which tends to increase a temperature, but I may also have a lowered immune function, which tends not to show higher temperature. As a result, I may not show high fever, but it's not because I don't have flu. It's much more because the symptoms of flu and the symptoms of lowered immune function are cancelling out each other. >> So, we also saw this in our initial exercise. We chose Thetadesis as the most parsimonious hypothesis for these data. But imagine if we didn't have that as an option. If we didn't have Thetadesis, we may have said that it's Betatosis, Iotalgia, and Kappacide, because the elevated A we see in Iotalgia cancels out the reduced A we see in Kappacide, which would account for our normal A levels. >> In general, cancel interactions are very hard to account for. In order to address these factors that make diagnosis so complex, it is useful to shift from the perspective of diagnostics solely as classification to a perspective of diagnostics as abduction.
 Let us look at abduction more closely. To understand the similarities and differences between deduction, abduction and induction, let us look at the relationship between a rule, cause and effect. Let's consider a simple rule like, if it cloudy, it rains. So it is cloudy, is the cause. It rains, is the effect. If it's cloudy and it rains, it's a rule. >> Another example of a rule, would be the rule that Bob hates Joe. So Bob hates Joe, so whenever Joe walks in, Bob leaves. The cause is Joe walking in. The effect is Bob leaving. And the rule is Bob hates Joe. You already come across yet another instance of this particular arrangement, when we were talking about flu and fever. So the rule might be; if flu then fever. Flue is the cause. Fever is the effect. Non-deduction, in one fundamental kind of inference. We know the rule and the cause, and we need to deduce the effect. So, given the rule, if it is cloudy, then it rains, and given the cause, it is cloudy, we can deduce that it rains. >> Similarly, in our rule that Bob hates Joe, we can say that if Joe just walked in, we can deduce that Bob will leave. Part of the rule is, if flu, then fever. And we know that a shortcut's flu, then we can deduce that, a shortcut's fever. This is simply an instance of more despondence, and we discussed this in detail when we were talking about logic. Now let us look at induction. Given a relationship between a cause and an effect, we can try to induce a rule. For example, if we observe repeatedly that when it is cloudy, it rains, and we may induce a rule. If it is cloudy, then it rains. >> Same thing with Bob and Joe. If we observe repeatedly that every time Joe arrives, Bob leaves. we can induce a rule that Bob must hate Joe. If every time a patient goes to a doctor with flu, and the patient has fever, then we can induce a rule if flu then fever. In can of abduction, given a rule and an effect, we can abduce a cause. As an example, given the rule if it is cloudy then it rains, and the effect that it is raining, we can ask of ourselves is it cloudy. And once again with Bob and Joe, given our rule that Bob hates Joe, and given that we just arrived at the party, and we see that Joe is here but not Bob, we might be able to adduce that Bob left when Joe arrived. >> Or given the rule, if flu then fever and that fact Ashok that has fever, we might be able to adduce that Ashok has flu. First of all notice that we are back to diagnosis. Diagnosis is an instance of abduction. But notice several of the properties. First, deduction is truth preserving. If the rule is true, and the cause is true, we can always guarantee that the effect is true as well. Induction and abduction are not truth-preserving. If we know something of the relations between cause and effect for some sample, that does not mean that the same relationship holds for the entire population. Induction does not always guarantee correctness. Same for abduction. We may know the rule and the effect, and we may suppose that the cause is true. But that need not necessarily be true. It may be the case the flu then fever. And Ashark may have fever, but that does not necessarily mean that a shark has flu. Fever can be caused by many many things. The reason that fever does not necessarily mean that Ashkk has flu is because there can be multiple causes for the same effect. Multiple hypothesis for the same data. This is exactly the problem that we had encountered earlier, when we were talking about what makes diagnosis hard. We said that deduction, induction, and abduction, are three of the fundamental forms of inference. We can of course also combine these inferences, science is a good example, you and I as scientists, observe some data about the world. Then we induced some explanation for it. Having induced such an explanation for it, we induce a rule. Having induced a rule, now we can use production to predict new data elements. We going to observe some more. Again, we adduce, induce, deduce. And we continue the cycle. Might the cycle also explain a significant part of cognition? Is this what you and I do on a daily basis? Adduce, induce, deduce?
 Now that we understand abduction, and now that we know the diagnosis is an instance of abduction, let us ask ourselves, how does this understanding help us in choosing hypotheses? So the first principle for choosing a hypothesis is explanatory coverage. A hypotheses must cover as much of the data as possible. Here's an example, hypotheses H3 explain data items D1 through D8. Hypothesis H7 explains data item D5 to D9. Assuming that all of these data elements are equally important or equally salient, we may prefer H3 over H7 because it explains for of the data than does H7. The second principle for choosing between competing hypotheses is called the principle of Parsimony. All things being equal, we want to pick the simplest explanation for the data. So consider the following scenario. H2 explains data elements D1 to D3. H4 explains data elements D1 through D8. H6 explains data elements D4 to D6 and H8 explains data elements D7 to D9. Now if you went by the criteria of explanatory coverage, then we might pick H2, plus H6, plus H8, because the three of them combined, explain more than just H4. However, the criteria of Parsimony would suggest if you pick H4, because H4 alone, explains almost all the data, and we don't need the other three hypothesis. In general this is a balancing act between these two principles. We want to both maximize the coverage, and maximize the parsimony. Based on this particular example, we may go with H4 and H8. The two together explain all the data and in addition, the set of these two hypotheses is smaller than these set of hypotheses H2, H6, and H8. The [UNKNOWN] criteria for choosing between competing hypotheses is that we want to pick those hypotheses in which we have more confidence. Some hypotheses are more likely than others. You may have more confidence in some hypotheses than in others. As an example, in this particular scenario, H3 may explain data items D1 to D8 and H5 may explain more data elements from D1 to D9. So H5 also explains D9 that H3 doesn't. However, we may have more confidence in H3, and so we may pick H3 instead of H5. Once again this is a balancing act between these three criteria for choosing between competing diagnostic hypotheses. A quick point to note here, these three criteria are useful for choosing between competing hypotheses even if the task is not diagnosis. The same problem occurs for example in intelligence analysis. Imagine that you have some data that needs to be explained and your competing hypothesis for explaining that particular data, well, you may pick between the competing hypothesis based on this criteria. All of the task is not a diagnostic task. These three criteria are useful for explanation. Diagnosis simply happens to be an example of this [UNKNOWN] task.
 Let us do an exercise together. The data in this particular exercise, a little bit more complicated than in the previous one. On the right-hand side, I've shown a set of diseases. What disease or subset of these diseases best explains the available data?
 What answer did you give, David?`` >> So, my answer is that the best explanation is a combination of Betatosis and Zetad. By combining these two illnesses, we can cover all of the data we saw over here. We saw both illnesses elevate B, which led to our High B, and both reduced C, which led to our Low C. Our patient had a normal level of E, but the effects of Betatosis and Zetad counteract their influence on E. Then Zetad accounts for our Reduced F, and Betatosis accounts for our Reduced H. My explanation, though, heavily weights the principle of coverage. I sacrifice parsimony by having two different explanations for the sake of coverage, I now cover all the symptoms. I could have also chose Thetadesis. Thetadesis would have explained B, C, and H. It wouldn't explain F, but it would be a simpler explanation than the combination of Betatosis and Zetad. In that case I would be sacrificing coverage for parsimony. I could also augment Thetadesis with Kappacide and Mutension. The three of them together would cover all our symptoms, but that would also be less parsimonious than Betatosis and Zetad alone. I may have chosen to do that though, if those three diseases were much more common than Betatosis and Zetad. In which case, I would have more confidence in them than just these two. >> It's an excellent answer, David, but how did you come up with this answer? >> So what I did, is I started with the data and I looked at which of these hypotheses currently matched the data best. That is to say, which hypotheses explained the most points and had the fewest conflicts. So for example, Betatosis explained the High B, the Low C, and the Low H, but it didn't explain the Low F, and it also suggested there was Elevated E. Initially, Thetadesis was the best match, it explained three symptoms, it just didn't explain the fourth. Then, based on that, I went looking for another illness that would complete the explanation. For Thetadesis, that ended up leading me to Kappacide and Mutension, but at that point, I was using three different hypotheses. So I decided to revisit one of the closer matches from my original round. I revisited Betatosis, which had two mismatches, it predicted Elevated E and didn't explain the Reduced F. And I went looking for another hypothesis that would complete that explanation. Zetad happened to have exactly those two effects. >> Note that one can use alternative methods for the same problem. For example, one could use case-based reasoning, and for it, we came across a problem very similar to this one previously. Suppose that the solution to that particular problem was of a level as a case. In that particular case, B was high, C was lower, F was low, and the solution was Thetadesis. In the current problem, the additional symptom is that F is low. So case would force you to the conclusion of Thetadesis. But it will create this particular solution to also account for the additional symptom of F being low. We could do that by adding Kappacide and Mutension to Thetadesis. Case-based reasoning does, will tend to forecast the alternative set of hypothesis. One more point to note here then, note that different methods can lead to different solutions. Given different methods, how might an error agent decide which method to select? We'll return to this particular problem when we discuss meta reasoning.
 Now we said earlier, the diagnosis is mapping from data space to hypothesis space. We also noted that although we may start with the initial set of data, proposals of specific hypotheses will lead us to collect additional data. Once we have some hypotheses for explaining the available data, then this hypotheses become indices into treatment plans. So the next step is to map the hypothesis space to the treatment space. In case of medical diagnosis, this might be a set of therapies, or a set of drugs. In case of auto mechanics the treatment space might consist of replacement of various parts or repair of various parts. Note here in the power of classification, this was the space of percepts, this was the space of actions, this mapping is very complicated. So we map the space of percepts into the equal classes in these catergories So we index the actions over the equal classes in these catergories so the data index the actions of the data space. As we exited the treatment, we monitor it. And depending upon the result of this treatment, we may need to collect additional data. The treatment fails. And the fact that the treatment has failed is important data that might also lead to the collection of additional data. Thus in case of auto mechanics, if I replace a particular component and the car still does not function properly, then that is useful data to know. Because it suggests that the fault that I thought would explain the malfunction is probably not the right diagnosis. >> We could also think of this last phase as a type of configuration, which we talked about last time. Given a set of hypotheses about illnesses or faults with a car, we can then configure a set of treatments or repairs that best address the faults we discovered before.
 So would the idea of diagnosis help us design an agent that can answer Raven's progressive matrices? Perhaps the best way to think about this is to consider how your agent might respond when it answers a question wrong. First, what data will it use to investigate its incorrect answer? Second, what hypotheses might it have for incorrect answers? Third, how will it select a hypothesis that best explains that data? And last, once it's selected hypothesis that explains that data, how will it use that to repair its reasoning, so it doesn't make the same mistake again?
 So today, we talked about diagnosis which is a term we're very familiar with from our everyday lives. But today, we talked about it specifically in a knowledge-based AI sense. We started off by defining diagnosis, which is finding the fault responsible for the malfunction in some system. This can be computers, computer programs, cars or even people and animals. We then talked about the process of diagnosis, mapping data onto hypotheses and how we can see this as a form of classification. We discovered though that this can be a very complicated process and classification might not get us all the way there. So then we talked about diagnosis as a form of abduction. Given a rule and effect or a symptom, we can abduce the cause of that problem, like an illness or a software bug. Both configuration and diagnosis have been small tasks in the broader process of design. Now that we talk about them, we can talk about AI agents that can actually do design in the real world, as well as what it would mean for an AI agent to really be creative.
 Diagnosis is a very common cognitive task. It occurs whenever our expectations are violated. We start diagnosing. Why were our expectations violated? Within a system, we expect some behavior out of it. We get a different behavior. Why did the system not give the behavior we expected from it? Notice that diagnosis is a task. We can use several methods to address it, like case-based reasoning. We have discussed diagnosis on several contexts like medicine, program debugging, car repair, but it's also very common in other aspects of our life. For example, you get unexpected traffic. Why did it occur? We review interaction with a co-worker or the economy. All are examples of diagnosis
 Please write down what you learned in this lesson.
 Thank you very much.
